\section{Entropy Unification}
\label{sec:entropy-unification}

\subsection{Three Manifestations of Entropy}

Entropy appears in three guises:
\begin{enumerate}
    \item \textbf{Thermodynamic entropy}: $S = k_B \ln W$ (Boltzmann)
    \item \textbf{Information entropy}: $H = -\sum p_i \log p_i$ (Shannon)
    \item \textbf{Categorical entropy}: $S_{\text{cat}} = k_B \ln C$ (partition counting)
\end{enumerate}

We prove these are mathematically equivalent for oscillatory systems.

\subsection{Oscillator Entropy}

\begin{definition}[Oscillator Microstate Count]
An oscillator with $n$ accessible energy levels has microstate count $W = n$.
\end{definition}

\begin{proposition}[Single Oscillator Entropy]
The entropy of a single oscillator with $n$ levels is
\begin{equation}
    S_{\text{osc}} = k_B \ln n
\end{equation}
\end{proposition}

\begin{theorem}[Multi-Oscillator Entropy]
For $M$ independent oscillators, each with $n$ accessible levels, the total entropy is
\begin{equation}
    S = k_B M \ln n
\end{equation}
\end{theorem}

\begin{proof}
Independence implies factorization: $W_{\text{total}} = n^M$. Therefore $S = k_B \ln(n^M) = k_B M \ln n$.
\end{proof}

\subsection{Information-Theoretic Entropy}

\begin{definition}[Shannon Entropy]
For a probability distribution $\{p_1, \ldots, p_n\}$ over $n$ outcomes,
\begin{equation}
    H = -\sum_{i=1}^n p_i \log_2 p_i \text{ bits}
\end{equation}
\end{definition}

\begin{proposition}[Maximum Shannon Entropy]
Maximum entropy $H_{\max} = \log_2 n$ is achieved when $p_i = 1/n$ for all $i$.
\end{proposition}

\begin{theorem}[Shannon-Boltzmann Equivalence]
\begin{equation}
    S = k_B \ln 2 \cdot H
\end{equation}
The conversion factor $k_B \ln 2$ translates between bits and natural units.
\end{theorem}

\subsection{Categorical Entropy}

\begin{definition}[Categorical Entropy]
For a system with $C(n) = 2n^2$ categorical states at depth $n$,
\begin{equation}
    S_{\text{cat}}(n) = k_B \ln C(n) = k_B \ln(2n^2) = k_B(2\ln n + \ln 2)
\end{equation}
\end{definition}

\begin{theorem}[Entropy Unification]
\label{thm:unification}
For an oscillatory system with partition depth $n$:
\begin{equation}
    S_{\text{osc}} = S_{\text{info}} \cdot k_B \ln 2 = S_{\text{cat}} - k_B \ln 2
\end{equation}
All three entropy measures are equivalent up to additive constants.
\end{theorem}

\begin{proof}
\begin{itemize}
    \item $S_{\text{osc}} = k_B \ln n$ (oscillator counting)
    \item $S_{\text{info}} = \log_2 n$ bits $= k_B \ln n$ (Shannon, in natural units)
    \item $S_{\text{cat}} = k_B \ln(2n^2) = k_B(2\ln n + \ln 2)$ (category counting)
\end{itemize}
The categorical entropy includes the $2n^2$ capacity factor; restricting to states at fixed $(l, m, s)$ recovers $S_{\text{osc}}$.
\end{proof}

\subsection{Entropy and Measurement}

\begin{theorem}[Measurement Entropy Cost]
Each partition coordinate measurement has entropy cost
\begin{equation}
    \Delta S \geq k_B \ln 2
\end{equation}
per bit of information gained.
\end{theorem}

\begin{proof}
By Landauer's principle, erasing one bit requires dissipating $k_B T \ln 2$ of heat. Measurement is equivalent to erasure of prior uncertainty. The entropy cost follows.
\end{proof}

\begin{corollary}[Complete Measurement Cost]
Measuring all partition coordinates $(n, l, m, s)$ at depth $n$ has total entropy cost
\begin{equation}
    \Delta S_{\text{total}} \geq k_B \ln(2n^2) \cdot \ln 2 = k_B(2\ln n + \ln 2)\ln 2
\end{equation}
\end{corollary}

\subsection{Entropy Production in Fragmentation}

\begin{proposition}[Fragmentation Entropy]
Fragmentation from depth $n_i$ to $n_f > n_i$ produces entropy
\begin{equation}
    \Delta S = k_B \ln \frac{C(n_f)}{C(n_i)} = 2k_B \ln \frac{n_f}{n_i}
\end{equation}
\end{proposition}

\begin{example}
Fragmentation from precursor ($n=1$) to secondary fragment ($n=3$):
\begin{equation}
    \Delta S = 2k_B \ln 3 \approx 2.20 k_B
\end{equation}
\end{example}

\subsection{Second Law in Partition Space}

\begin{theorem}[Categorical Second Law]
\label{thm:second-law}
The total categorical entropy of an isolated system is non-decreasing:
\begin{equation}
    \frac{dS_{\text{cat}}}{dt} \geq 0
\end{equation}
\end{theorem}

\begin{proof}
Categorical transitions satisfy selection rules that constrain pathways. In an isolated system, transitions occur to states of equal or higher degeneracy (by detailed balance). Total categorical count $C$ is non-decreasing, hence $S_{\text{cat}} = k_B \ln C$ is non-decreasing.
\end{proof}

This categorical second law provides a thermodynamic arrow of time for partition coordinate dynamics: systems evolve toward states of higher categorical entropy.

