\section{Entropy from Triple Equivalence}

\subsection{The Physical Content of Mathematical Equivalence}

Section 1 established that bounded physical systems admit three mathematically equivalent descriptions: oscillatory, categorical, and partition. Each uses different mathematical objects (continuous functions, discrete sets, combinatorial operations) and different conceptual frameworks (dynamics, logic, arithmetic).

A fundamental question: does this mathematical equivalence extend to physical quantities? Specifically, does entropy—the central quantity of statistical mechanics and thermodynamics—have the same value when computed from each perspective?

We demonstrate that the answer is yes. Entropy emerges naturally from each perspective without additional statistical assumptions. The three derivations are independent, yet yield identical results. This convergence follows directly from the triple equivalence and the mandatory convergence principle (Section 1.5).

\subsection{The Pendulum as Prototype System}

We use the simple pendulum as our prototype bounded system. The pendulum satisfies all boundedness conditions:
\begin{itemize}
\item \textbf{Spatial:} Angular displacement $\theta \in [-\theta_{\max}, \theta_{\max}]$ where $\theta_{\max}$ is determined by energy
\item \textbf{Energetic:} Total energy $E = \frac{1}{2}mL^2\dot{\theta}^2 + mgL(1-\cos\theta) \leq E_{\max}$
\item \textbf{Temporal:} Period $T = 2\pi/\omega$ where $\omega = \sqrt{g/L}$ for small oscillations
\end{itemize}

Results derived for the pendulum generalize immediately to arbitrary bounded systems through the Poincaré recurrence theorem.

\subsection{Entropy from Oscillatory Mechanics}

\subsubsection{Phase Space Structure}

The oscillatory description characterizes the pendulum through continuous variables $(\theta, \dot{\theta})$ in phase space. Over one complete period $T$, the system traces a closed trajectory. The area enclosed by this trajectory is the accessible phase space volume $\Omega$.

For a harmonic oscillator with energy $E$:
\begin{equation}
\Omega = \oint p \, dq = 2\pi E/\omega
\end{equation}

For a pendulum with amplitude $A$ (maximum angular displacement):
\begin{equation}
E = \frac{1}{2}mL^2\omega^2 A^2 \implies \Omega = \pi mL^2\omega A^2
\end{equation}

\subsubsection{State Counting from Temporal Resolution}

To observe the pendulum with temporal resolution $\tau$, sample its state at discrete times $t_k = k\tau$ for $k \in \{0, 1, \ldots, n-1\}$ where $n = T/\tau$.

Each sample captures a distinguishable phase. Two samples separated by time $\tau$ correspond to different points on the phase space trajectory. The number of distinguishable states equals the number of samples: $n = T/\tau$.

Equivalently, through phase space volume: each distinguishable state occupies a cell of size $\Delta\Omega = \Omega/n$, giving:
\begin{equation}
n = \frac{\Omega}{\Delta\Omega}
\end{equation}

For quantum systems, the natural cell size is $\Delta\Omega = 2\pi\hbar$ (one quantum state). For classical systems, $\Delta\Omega$ is determined by measurement resolution.

\subsubsection{Oscillatory Entropy}

Define oscillatory entropy as the logarithm of distinguishable states:
\begin{equation}
S_{\text{osc}} = k_B \ln n
\end{equation}

This is the Boltzmann entropy formula. It counts configurations consistent with macroscopic constraints (energy $E$, period $T$).

For a quantum oscillator with $n = \Omega/(2\pi\hbar)$:
\begin{equation}
S_{\text{osc}} = k_B \ln\left(\frac{\Omega}{2\pi\hbar}\right) = k_B \ln\left(\frac{E}{\hbar\omega}\right)
\end{equation}

For $M$ independent oscillatory modes (e.g., $M$ coupled pendulums or $M$ degrees of freedom), each contributes independently:
\begin{equation}
S_{\text{osc}} = k_B \sum_{i=1}^{M} \ln n_i = k_B M \ln\langle n\rangle
\end{equation}
where $\langle n\rangle$ is the average quantum number per mode.

\subsubsection{Physical Interpretation}

Oscillatory entropy measures phase space accessibility. A pendulum with large amplitude (high energy) has large $\Omega$, hence large $n$, hence large entropy. A pendulum with small amplitude (low energy) has small $\Omega$, small $n$, small entropy.

Crucially, this entropy depends only on macroscopic observables (energy $E$, period $T$, resolution $\tau$), not on microscopic trajectory details. Two pendulums with the same $E$ and $T$ have identical oscillatory entropy regardless of initial conditions.

\subsection{Entropy from Categorical Structure}

\subsubsection{Category Construction}

The categorical description divides the pendulum's period into discrete temporal intervals. Define $n$ categories corresponding to time windows:
\begin{equation}
C_k = \left\{\text{states with } t \in \left[k\frac{T}{n}, (k+1)\frac{T}{n}\right)\right\}, \quad k \in \{0, 1, \ldots, n-1\}
\end{equation}

Each category is a subset of phase space containing all states accessible during the $k$-th interval. The pendulum trajectory induces a deterministic sequence:
\begin{equation}
C_0 \to C_1 \to C_2 \to \cdots \to C_{n-1} \to C_0
\end{equation}

Categories are mutually exclusive (no overlap) and exhaustive (every state belongs to exactly one category).

\subsubsection{Microstate Counting}

Within each category $C_k$, there may be multiple distinguishable microstates. Define the \textit{category depth} $d_k$ as the number of distinguishable microstates within $C_k$:
\begin{equation}
d_k = \frac{\text{Phase space volume of } C_k}{\text{Resolution cell size}}
\end{equation}

For uniform sampling, all categories have equal depth $d_k = d$. The total number of distinguishable microstates is:
\begin{equation}
\Omega_{\text{cat}} = \sum_{k=0}^{n-1} d_k = nd
\end{equation}

If we distinguish only temporal categories (not internal structure), then $d = 1$ and $\Omega_{\text{cat}} = n$.

\subsubsection{Categorical Entropy}

Define categorical entropy as the logarithm of total microstates:
\begin{equation}
S_{\text{cat}} = k_B \ln \Omega_{\text{cat}}
\end{equation}

For uniform depth $d = 1$ (temporal categories only):
\begin{equation}
S_{\text{cat}} = k_B \ln n
\end{equation}

For non-uniform depth:
\begin{equation}
S_{\text{cat}} = k_B \ln(nd) = k_B \ln n + k_B \ln d
\end{equation}

The first term counts temporal categories; the second counts internal structure.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/fig_pendulum_triple_equivalence.png}
    \caption{\textbf{Pendulum as Prototype System: Triple Equivalence Demonstrated Through Oscillatory, Categorical, and Partition Views.} 
    \textbf{Top Left (Oscillatory View):} Pendulum with pivot (black dot) and bob (large black circle) swinging with angular displacement $\theta(t) = \theta_{\max} \cos(\omega t)$. Gray lines show trajectory envelope. This is the \emph{continuous} description: the pendulum traces a smooth sinusoidal path in time.
    \textbf{Top Middle (Continuous Periodic Motion):} Two plots showing $\theta(t)$ (solid blue) and $\dot{\theta}(t)$ (dashed blue) vs. time. Angular displacement oscillates between $\pm 0.4$ rad with period $T$. Angular velocity is $90°$ out of phase (peaks when displacement crosses zero). The continuous curves demonstrate smooth oscillatory dynamics.
    \textbf{Top Right (Phase Space Ellipse):} Trajectory in $(\theta, \dot{\theta})$ space forms a closed ellipse (blue curve). Two red dots mark the current state and a previous state, showing clockwise traversal. The ellipse demonstrates that the pendulum occupies a bounded region of phase space with area $\propto$ energy.
    \textbf{Bottom Left (Categorical View):} Eight categories $C_1, \ldots, C_8$ (green circles) arranged in a semicircle, connected to a central node (black dot). Each $C_i$ is a \emph{distinguishable state} (discrete partition of the angular range). This is the \emph{categorical} description: the pendulum's position is discretized into $M=8$ bins.
    \textbf{Bottom Middle (Discrete State Structure):} Bar chart showing time spent in each category. Two tall bars at $C_3$ and $C_4$ (time $\approx 0.8$) indicate that the pendulum spends most time near the turning points (where $\dot{\theta} = 0$, velocity is minimum). Short bars at $C_1$ and $C_8$ (time $\approx 0.2$) indicate fast traversal through the center (where $|\dot{\theta}|$ is maximum). Horizontal arrow labeled "Traversal" indicates the direction of category transitions. This demonstrates that categorical traversal is \emph{non-uniform}: the system dwells longer in some categories than others.
    \textbf{Bottom Right (Partition View):} Eight partitions $P_1, \ldots, P_8$ (pink rectangles) arranged horizontally along the time axis. Each partition represents one category transition, with width $\langle \tau_p \rangle = T/M$ (average dwell time). Total period $T = \sum_{i=1}^M \tau_i$ (sum of individual dwell times). This is the \emph{partition} description: time is divided into discrete intervals corresponding to category occupations.
    \textbf{Bottom (Triple Equivalence Statement):} Yellow box states: \textbf{Oscillation = Category Traversal = Period Partition}. Fundamental identity: $\frac{dM}{dt} = \omega = \frac{2\pi}{T/M} = \frac{1}{\langle \tau_p \rangle}$. This equation unifies the three descriptions: \textbf{(1)} Oscillatory: angular frequency $\omega = 2\pi / T$, \textbf{(2)} Categorical: category transition rate $dM/dt = M/T$, \textbf{(3)} Partition: inverse dwell time $1/\langle \tau_p \rangle = M/T$.}
    \label{fig:pendulum_triple_equivalence}
    \end{figure}
    

\subsubsection{Information-Theoretic Formulation}

An equivalent formulation uses Shannon entropy. If the system can occupy any of $n$ categories with probabilities $p_k$, the entropy is:
\begin{equation}
S_{\text{cat}} = -k_B \sum_{k=0}^{n-1} p_k \ln p_k
\end{equation}

For uniform sampling (equal time in each category), $p_k = 1/n$, giving:
\begin{equation}
S_{\text{cat}} = -k_B \sum_{k=0}^{n-1} \frac{1}{n} \ln\frac{1}{n} = k_B \ln n
\end{equation}

This recovers the microstate counting result.

\subsubsection{Physical Interpretation}

Categorical entropy measures distinguishable configurations. Fine temporal resolution ($\tau$ small, $n$ large) yields many categories, hence large entropy. Coarse resolution ($\tau$ large, $n$ small) yields few categories, hence small entropy.

This entropy depends on observational resolution $\tau$. Two observers with different resolutions compute different categorical entropies for the same pendulum. However, observers using the same resolution compute identical entropy—this is the convergence property.

\subsection{Entropy from Partition Operations}

\subsubsection{Partition Construction}

The partition description constructs temporal structure through recursive subdivision. Begin with the full period $[0, T]$. Apply partition operation $\Pi_n$ dividing this into $n$ equal parts:
\begin{equation}
\Pi_n: [0, T] \to \bigcup_{k=0}^{n-1} \left[k\frac{T}{n}, (k+1)\frac{T}{n}\right]
\end{equation}

Each interval $[k\tau, (k+1)\tau]$ where $\tau = T/n$ is a \textit{partition cell}. The partition depth is $n$. The partition width is $\tau$.

\subsubsection{Combinatorial Structure}

Partition cells have additive structure. Any interval $[0, k\tau]$ is constructed by concatenating $k$ elementary cells:
\begin{equation}
[0, k\tau] = \bigcup_{j=0}^{k-1} [j\tau, (j+1)\tau]
\end{equation}

Using additive notation with $\tau_1 = \tau$:
\begin{equation}
t_k = k\tau_1 = \underbrace{\tau_1 + \tau_1 + \cdots + \tau_1}_{k \text{ times}}
\end{equation}

This additive structure is fundamental: partition cells can be combined (concatenated) and decomposed (subdivided) using arithmetic operations.

\subsubsection{Distinguishable Partition Counting}

To define partition entropy, count distinguishable ways to partition the period. Two partitions are distinguishable if they differ observably.

For a pendulum with $n$ distinguishable positions (one per time interval), there are $n$ distinguishable partitions:
\begin{equation}
\Omega_{\text{part}} = n
\end{equation}

Taking the logarithm:
\begin{equation}
S_{\text{part}} = k_B \ln n
\end{equation}

\subsubsection{Selectivity Formulation}

An alternative approach uses partition selectivity. Each partition cell "selects" a subset of phase space. If cell $k$ has selectivity $s_k$ (fraction of phase space it contains), the total configuration count is:
\begin{equation}
\Omega_{\text{part}} = \prod_{k=0}^{n-1} \frac{1}{s_k}
\end{equation}

For uniform partitioning, $s_k = 1/n$, giving:
\begin{equation}
\Omega_{\text{part}} = \left(\frac{1}{1/n}\right)^n = n^n
\end{equation}

But this overcounts. The correct counting recognizes that we enumerate distinguishable partition configurations, not independent cell selections. For $n$ cells with uniform selectivity, there are $n$ distinguishable configurations:
\begin{equation}
\Omega_{\text{part}} = n
\end{equation}

Therefore:
\begin{equation}
S_{\text{part}} = k_B \ln n
\end{equation}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/panel_aperture_selectivity.png}
    \caption{\textbf{Aperture Selectivity Determines Transport Coefficients.} 
    All four transport coefficients are expressed as $\Xi = N^{-1} \sum_{i,j} \tau_{p,ij} g_{ij}$, where $\tau_p$ is partition lag and $g$ is coupling strength. 
    (\textbf{Top left}) Electrical resistivity $\rho = N^{-1} \sum \tau_{s,ij} g_{ij}$ as a function of temperature. Copper (orange) has low resistivity ($\sim 10^{-8}~\Omega\cdot$m) that increases linearly with temperature due to phonon scattering. Superconductors (cyan) exhibit zero resistivity below $T_c$ due to aperture bypass by Cooper pairs. Insulators (gray) have extremely high resistivity ($> 10^{10}~\Omega\cdot$m). 
    (\textbf{Top right}) Viscosity $\mu = \sum \tau_{p,ij} g_{ij}$ as a function of temperature. Water (cyan) has low viscosity ($\sim 1$ mPa$\cdot$s) that decreases with temperature. Glycerol (green) has high viscosity ($\sim 10^3$ mPa$\cdot$s) due to strong intermolecular coupling. Superfluid helium (magenta) has zero viscosity below $T_\lambda$ due to quantum aperture bypass. 
    (\textbf{Bottom left}) Diffusivity $D = (k_B T)^{-1} \sum \tau_{p,ij}^{-1} g_{ij}^{-1}$ as a function of temperature. Carbon in copper (orange) has high diffusivity that increases exponentially with temperature. Carbon in iron (red) has lower diffusivity. Hydrogen in palladium (green) has the highest diffusivity due to small atomic size and weak coupling. 
    (\textbf{Bottom right}) Thermal conductivity $\kappa = \sum \tau_{p,ij}^{-1} g_{ij}$ as a function of temperature. Diamond (cyan) has the highest thermal conductivity ($> 10^3$ W/m$\cdot$K) due to strong covalent bonds and long phonon mean free paths. Metals (copper, green) have intermediate conductivity ($\sim 10^2$ W/m$\cdot$K). Insulators (silicon, glass) have low conductivity ($< 10$ W/m$\cdot$K). 
    The unified formula demonstrates that all transport coefficients arise from the same partition-coupling structure, with different combinations of $\tau_p$ and $g$ producing the diverse behaviors observed in nature.}
    \label{fig:aperture_selectivity}
    \end{figure*}

\subsubsection{Physical Interpretation}

Partition entropy measures distinguishable subdivision schemes. A finely partitioned period ($n$ large) has more distinguishable configurations than a coarsely partitioned period ($n$ small).

The partition perspective emphasizes combinatorial structure: entropy arises from counting distinguishable arrangements, not from probabilistic assumptions. This is the most fundamental view—oscillatory and categorical entropies derive from partition entropy by identifying states and categories with partition cells.

\subsection{Entropy Equivalence Theorem}

Three independent derivations yield:
\begin{align}
S_{\text{osc}} &= k_B \ln n \quad \text{(phase space volume)} \\
S_{\text{cat}} &= k_B \ln n \quad \text{(microstate count)} \\
S_{\text{part}} &= k_B \ln n \quad \text{(configuration count)}
\end{align}

\begin{theorem}[Entropy Equivalence]
\label{thm:entropy_equivalence}
For any bounded system with temporal resolution $\tau = T/n$, entropy computed from oscillatory, categorical, and partition descriptions is identical:
\begin{equation}
S_{\text{osc}} = S_{\text{cat}} = S_{\text{part}} = k_B \ln n
\end{equation}
\end{theorem}

\begin{proof}
By the triple equivalence (Theorem 1.1), the three descriptions are related by bijections:
\begin{equation}
\text{Oscillatory state } t_k \leftrightarrow \text{Category } C_k \leftrightarrow \text{Partition cell } [k\tau, (k+1)\tau]
\end{equation}

Bijections preserve cardinality. Therefore:
\begin{equation}
n_{\text{osc}} = n_{\text{cat}} = n_{\text{part}} = n
\end{equation}

Entropy is the logarithm of state count (Boltzmann formula):
\begin{equation}
S = k_B \ln(\text{number of states})
\end{equation}

Therefore:
\begin{align}
S_{\text{osc}} &= k_B \ln n_{\text{osc}} = k_B \ln n \\
S_{\text{cat}} &= k_B \ln n_{\text{cat}} = k_B \ln n \\
S_{\text{part}} &= k_B \ln n_{\text{part}} = k_B \ln n
\end{align}

All three entropies are equal.
\end{proof}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/panel2_entropy_derivation.png}
    \caption{\textbf{Entropy Derivation: Three Methods (Oscillatory, Categorical, Partition) Yield Identical Formula $S = k_B M \ln n$.} 
    (\textbf{A}) Oscillatory: counting quantum states. Three modes (Mode 1, 2, 3) each with $n = 4$ states ($|0\rangle, |1\rangle, |2\rangle, |3\rangle$, colored boxes: light orange, orange, red, dark red). Total microstates $W_{\text{osc}} = 4^3 = 64$ states—each mode independent, total states = product. Grid shows all possible combinations. 
    (\textbf{B}) Categorical: counting distinguishable states. Two dimensions ($C_1, C_2$) each with 4 categories ($C_{1,1}, C_{1,2}, C_{1,3}, C_{1,4}$ and $C_{2,1}, C_{2,2}, C_{2,3}, C_{2,4}$, green circles). Total states $|C| = 4 \times 4 = 16$ states for $M = 2$—demonstrates categorical product structure. Grid shows 4$\times$4 array of green circles representing all distinguishable configurations. 
    (\textbf{C}) Partition: counting paths through tree. Binary tree with 3 levels (Level 0, 1, 2) and branching $n = 3$. Root (blue circle at Level 0) splits into 3 branches at Level 1 (blue circles), each splitting into 3 branches at Level 2 (blue circles), yielding $3^2 = 9$ leaves (terminal blue circles). One path highlighted in red shows specific trajectory through tree. Paths $= n \times n \times \cdots = n^M$—leaves = $3^2 = 9$ paths. 
    (\textbf{D}) Boltzmann's fundamental relation: box shows derivation. Start with $S = k_B \ln W$ (entropy = Boltzmann constant $\times$ log of microstates). Substitute $W = n^M$: $S = k_B \ln(n^M)$. Apply logarithm property: $S = k_B \cdot M \ln n$. Final result in purple box: $S = k_B M \ln n$—demonstrates that entropy scales linearly with degrees of freedom $M$ and logarithmically with states per DOF $n$. 
    (\textbf{E}) Three derivations, one formula: three colored boxes connected by arrows. Oscillators (red): $W_{\text{osc}} = n^M$. Categorical (green): $|C| = n^M$. Partition (blue): $P = n^M$. All converge to black box: "All give: $W = n^M$", leading to purple box: $S = k_B M \ln n$—validates mathematical equivalence of three counting methods. 
    (\textbf{F}) Entropy scaling $S = k_B M \ln n$: 2D heatmap shows $S/k_B$ (color scale 0-24, purple-teal-yellow) vs degrees of freedom $M$ (horizontal, 2-10) and states per DOF $n$ (vertical, 2-10). White dashed contour lines show constant entropy curves: $S = \ln n$ (logarithmic, bottom), $S \times \ln n$ (linear), increasing to top-right corner. Red star labeled "IDEAL" at $(M, n) = (4, 4)$ with $S/k_B \sim 5.5$. Yellow star labeled "Gas: $M = 3$, $n = 6$" (linear) at $(M, n) = (3, 6)$ with $S/k_B \sim 5.4$. Green circle labeled "Pendulum: $M = 1$, $n = 4$" at $(M, n) = (1, 4)$ with $S/k_B \sim 1.4$. Entropy increases linearly along horizontal (more DOFs) and logarithmically along vertical (more states per DOF)—demonstrates that adding DOFs is more efficient for increasing entropy than adding states, explaining why high-dimensional systems have greater capacity for information storage and thermodynamic complexity.}
    \label{fig:entropy_derivation}
    \end{figure}

\subsection{Extension to Multiple Degrees of Freedom}

\subsubsection{Independent Oscillators}

Consider $M$ independent pendulums, each with a period $T_i$ and a resolution $\tau_i$, giving a state count $n_i = T_i/\tau_i$. The total phase space is the Cartesian product:
\begin{equation}
\Omega_{\text{total}} = \prod_{i=1}^{M} n_i
\end{equation}

Taking the logarithm:
\begin{equation}
S_{\text{total}} = k_B \ln\left(\prod_{i=1}^{M} n_i\right) = k_B \sum_{i=1}^{M} \ln n_i
\end{equation}

For identical oscillators ($n_i = n$):
\begin{equation}
S_{\text{total}} = k_B M \ln n
\end{equation}

\subsubsection{Indistinguishable Particles}

For $N$ indistinguishable oscillators (identical gas molecules), divide by $N!$ to avoid overcounting:
\begin{equation}
\Omega_{\text{indist}} = \frac{n^N}{N!}
\end{equation}

Using Stirling's approximation $\ln N! \approx N\ln N - N$:
\begin{equation}
S_{\text{indist}} = k_B \ln\left(\frac{n^N}{N!}\right) = k_B N\ln\left(\frac{en}{N}\right)
\end{equation}

For $n \gg N$ (dilute limit):
\begin{equation}
S_{\text{indist}} \approx k_B N \ln n
\end{equation}

This is the Sackur-Tetrode formula for ideal gas entropy.

\subsubsection{Coupled Oscillators}

For coupled oscillators (atoms in a solid), normal modes become the relevant degrees of freedom. Each normal mode contributes $k_B \ln n_i$ where $n_i$ is the excitation level:
\begin{equation}
S_{\text{coupled}} = k_B \sum_{i=1}^{M} \ln n_i
\end{equation}

This is the Debye model (phonons) or Planck distribution (photons).

\subsection{Connection to Thermodynamic Entropy}

\subsubsection{Thermodynamic Definition}

Classical thermodynamics defines entropy through reversible heat transfer:
\begin{equation}
dS_{\text{thermo}} = \frac{\delta Q_{\text{rev}}}{T}
\end{equation}

For a system with energy $E$ and temperature $T$:
\begin{equation}
S_{\text{thermo}} = \int \frac{dE}{T}
\end{equation}

\subsubsection{Statistical Temperature}

From oscillatory mechanics, energy relates to quantum number:
\begin{equation}
E = \hbar\omega n \implies n = \frac{E}{\hbar\omega}
\end{equation}

Statistical entropy:
\begin{equation}
S_{\text{stat}} = k_B \ln n = k_B \ln\left(\frac{E}{\hbar\omega}\right)
\end{equation}

Taking the derivative:
\begin{equation}
\frac{\partial S_{\text{stat}}}{\partial E} = \frac{k_B}{E}
\end{equation}

By thermodynamic definition, $\partial S/\partial E = 1/T$. Therefore:
\begin{equation}
\frac{1}{T} = \frac{k_B}{E} \implies T = \frac{E}{k_B}
\end{equation}

This defines temperature statistically: energy per degree of freedom (in units of $k_B$).

For $M$ degrees of freedom with total energy $U = ME$:
\begin{equation}
T = \frac{U}{Mk_B}
\end{equation}

Substituting back:
\begin{equation}
S_{\text{stat}} = k_B M \ln\left(\frac{T k_B}{\hbar\omega}\right) = Mk_B \ln T + \text{const}
\end{equation}

This matches thermodynamic entropy $S_{\text{thermo}} = \int (Mk_B/T) dT = Mk_B \ln T + \text{const}$.

Statistical and thermodynamic entropy are identical up to conventional additive constants.

\subsection{Resolution Dependence and Observer Independence}

\subsubsection{Apparent Resolution Dependence}

Two observers with different temporal resolutions $\tau_1$ and $\tau_2$ compute different state counts:
\begin{equation}
n_1 = \frac{T}{\tau_1}, \quad n_2 = \frac{T}{\tau_2}
\end{equation}

Therefore different entropies:
\begin{equation}
S_1 = k_B \ln n_1, \quad S_2 = k_B \ln n_2
\end{equation}

The difference:
\begin{equation}
S_2 - S_1 = k_B \ln\left(\frac{\tau_1}{\tau_2}\right)
\end{equation}

\subsubsection{Entropy Differences Are Observer-Independent}

If both observers measure the same system in two states (A and B):
\begin{align}
\Delta S_1 &= S_1^B - S_1^A = k_B \ln\left(\frac{n_1^B}{n_1^A}\right) \\
\Delta S_2 &= S_2^B - S_2^A = k_B \ln\left(\frac{n_2^B}{n_2^A}\right)
\end{align}

If both use consistent resolution:
\begin{equation}
\frac{n_1^B}{n_1^A} = \frac{T^B/\tau_1}{T^A/\tau_1} = \frac{T^B}{T^A} = \frac{T^B/\tau_2}{T^A/\tau_2} = \frac{n_2^B}{n_2^A}
\end{equation}

Therefore:
\begin{equation}
\Delta S_1 = \Delta S_2
\end{equation}

Entropy differences are observer-independent, confirming mandatory convergence.

\subsubsection{Physical Significance}

Resolution-dependence of absolute entropy reflects that entropy measures information content relative to measurement resolution. Resolution dependence of absolute entropy reflects that entropy measures information content relative to measurement resolution.

Physically relevant quantities are entropy differences (determining heat flow, work extraction), which are observer-independent as required by the objectivity criterion (Section 1.4).

\subsection{Summary}

We have derived entropy from three independent perspectives:

\textbf{Oscillatory:} Phase space volume $\Omega$ → state count $n = \Omega/(2\pi\hbar)$ → entropy $S = k_B \ln n$

\textbf{Categorical:} Temporal categories $C_k$ → microstate count $n$ → entropy $S = k_B \ln n$

\textbf{Partition:} Partition cells → configuration count $n$ → entropy $S = k_B \ln n$

All three yield $S = k_B \ln n$. Convergence is exact, not approximate—a mathematical necessity from the triple equivalence.

\begin{corollary}[Entropy as Intrinsic Property]
Any complete description of a bounded system yields identical entropy. Entropy is an intrinsic property of the system, not the description.
\end{corollary}

This establishes that entropy—the central quantity of thermodynamics—emerges identically from oscillatory, categorical, and partition perspectives. Subsequent sections extend this result to other thermodynamic quantities (temperature, pressure, free energy), demonstrating that thermodynamic structure follows from the triple equivalence.
