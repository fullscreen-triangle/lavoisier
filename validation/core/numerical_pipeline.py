#!/usr/bin/env python3
"""
Numerical Pipeline Orchestrator - Standalone Implementation
Main numerical processing pipeline for mass spectrometry data validation
"""

import os
import sys
import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import time
import warnings
from collections import defaultdict

# Import our standalone components
from .mzml_reader import StandaloneMzMLReader, Spectrum, load_mzml_file

warnings.filterwarnings("ignore")


@dataclass
class AnnotationResult:
    """Result from database annotation"""
    compound_id: str
    compound_name: str
    formula: str
    exact_mass: float
    database: str
    confidence_score: float
    match_type: str  # 'exact', 'isotope', 'fragment', 'similarity'
    metadata: Dict[str, Any]


@dataclass
class SpectrumEmbedding:
    """Spectrum embedding representation"""
    spectrum_id: str
    embedding_vector: np.ndarray
    embedding_method: str  # 'spec2vec', 'stellas', 'fingerprint'
    dimension: int
    metadata: Dict[str, Any]


class DatabaseSearchEngine:
    """Standalone database search implementation"""

    def __init__(self):
        self.databases = {
            'LIPIDMAPS': self._init_lipidmaps_db(),
            'MSLIPIDS': self._init_mslipids_db(),
            'PUBCHEM': self._init_pubchem_db(),
            'METLIN': self._init_metlin_db(),
            'MASSBANK': self._init_massbank_db(),
            'MZCLOUD': self._init_mzcloud_db(),
            'KEGG': self._init_kegg_db(),
            'HUMANCYC': self._init_humancyc_db()
        }

    def _init_lipidmaps_db(self) -> Dict[str, Any]:
        """Initialize LipidMaps database (simplified)"""
        # In real implementation, this would load from actual database files
        return {
            'name': 'LIPIDMAPS',
            'compounds': self._generate_synthetic_lipid_db(),
            'search_params': {'mz_tolerance': 0.01, 'rt_tolerance': 0.5}
        }

    def _init_mslipids_db(self) -> Dict[str, Any]:
        """Initialize MS-LIPIDS database"""
        return {
            'name': 'MS-LIPIDS',
            'compounds': self._generate_synthetic_lipid_db(prefix='MSLIP'),
            'search_params': {'mz_tolerance': 0.005, 'rt_tolerance': 0.3}
        }

    def _init_pubchem_db(self) -> Dict[str, Any]:
        """Initialize PubChem database"""
        return {
            'name': 'PUBCHEM',
            'compounds': self._generate_synthetic_metabolite_db(),
            'search_params': {'mz_tolerance': 0.02, 'rt_tolerance': 1.0}
        }

    def _init_metlin_db(self) -> Dict[str, Any]:
        """Initialize METLIN database"""
        return {
            'name': 'METLIN',
            'compounds': self._generate_synthetic_metabolite_db(prefix='METLIN'),
            'search_params': {'mz_tolerance': 0.01, 'rt_tolerance': 0.5}
        }

    def _init_massbank_db(self) -> Dict[str, Any]:
        """Initialize MassBank database"""
        return {
            'name': 'MASSBANK',
            'compounds': self._generate_synthetic_spectral_db(),
            'search_params': {'spectral_threshold': 0.7}
        }

    def _init_mzcloud_db(self) -> Dict[str, Any]:
        """Initialize mzCloud database"""
        return {
            'name': 'MZCLOUD',
            'compounds': self._generate_synthetic_spectral_db(prefix='MZCLOUD'),
            'search_params': {'spectral_threshold': 0.8}
        }

    def _init_kegg_db(self) -> Dict[str, Any]:
        """Initialize KEGG database"""
        return {
            'name': 'KEGG',
            'compounds': self._generate_synthetic_pathway_db(),
            'search_params': {'mz_tolerance': 0.015, 'pathway_weight': 1.5}
        }

    def _init_humancyc_db(self) -> Dict[str, Any]:
        """Initialize HumanCyc database"""
        return {
            'name': 'HUMANCYC',
            'compounds': self._generate_synthetic_pathway_db(prefix='HC'),
            'search_params': {'mz_tolerance': 0.01, 'pathway_weight': 2.0}
        }

    def _generate_synthetic_lipid_db(self, prefix: str = 'LM') -> List[Dict[str, Any]]:
        """Generate synthetic lipid database"""
        lipid_classes = ['PC', 'PE', 'PS', 'PA', 'PG', 'PI', 'TG', 'DG', 'MG', 'CE', 'SM', 'LPC', 'LPE']
        compounds = []

        for i, lipid_class in enumerate(lipid_classes):
            for j in range(50):  # 50 compounds per class
                compound_id = f"{prefix}_{lipid_class}_{j:03d}"
                exact_mass = 200 + i * 50 + j * 2 + np.random.normal(0, 5)

                compound = {
                    'id': compound_id,
                    'name': f"{lipid_class}({16 + j//10}:{j%4}/0:0)" if lipid_class in ['PC', 'PE'] else f"{lipid_class}_{j}",
                    'formula': f"C{20+j}H{40+j*2}NO{4+i}P" if lipid_class in ['PC', 'PE'] else f"C{15+j}H{30+j}O{3+i}",
                    'exact_mass': max(100, exact_mass),
                    'lipid_class': lipid_class,
                    'retention_time_predicted': 5.0 + j * 0.1 + np.random.normal(0, 0.5)
                }
                compounds.append(compound)

        return compounds

    def _generate_synthetic_metabolite_db(self, prefix: str = 'PC') -> List[Dict[str, Any]]:
        """Generate synthetic metabolite database"""
        metabolite_classes = ['amino_acid', 'sugar', 'organic_acid', 'nucleotide', 'vitamin']
        compounds = []

        for i, met_class in enumerate(metabolite_classes):
            for j in range(30):
                compound_id = f"{prefix}_{met_class}_{j:03d}"
                exact_mass = 100 + i * 30 + j * 3 + np.random.normal(0, 10)

                compound = {
                    'id': compound_id,
                    'name': f"{met_class}_compound_{j}",
                    'formula': f"C{5+j}H{10+j*2}N{1+i%3}O{2+i}",
                    'exact_mass': max(50, exact_mass),
                    'metabolite_class': met_class,
                    'retention_time_predicted': 1.0 + j * 0.05
                }
                compounds.append(compound)

        return compounds

    def _generate_synthetic_spectral_db(self, prefix: str = 'MB') -> List[Dict[str, Any]]:
        """Generate synthetic spectral database"""
        compounds = []

        for i in range(200):
            compound_id = f"{prefix}_{i:04d}"
            exact_mass = 150 + i * 5 + np.random.normal(0, 20)

            # Generate synthetic fragment spectrum
            n_fragments = np.random.randint(10, 50)
            fragment_mzs = np.sort(np.random.uniform(50, exact_mass, n_fragments))
            fragment_intensities = np.random.exponential(1000, n_fragments)

            compound = {
                'id': compound_id,
                'name': f"compound_{i}",
                'formula': f"C{8+i//10}H{15+i//5}NO{3+i//20}",
                'exact_mass': max(100, exact_mass),
                'spectrum': {
                    'mz': fragment_mzs.tolist(),
                    'intensity': fragment_intensities.tolist()
                },
                'spectrum_type': 'MS2'
            }
            compounds.append(compound)

        return compounds

    def _generate_synthetic_pathway_db(self, prefix: str = 'KEGG') -> List[Dict[str, Any]]:
        """Generate synthetic pathway database"""
        pathways = ['glycolysis', 'tca_cycle', 'fatty_acid_synthesis', 'amino_acid_metabolism', 'nucleotide_metabolism']
        compounds = []

        for i, pathway in enumerate(pathways):
            for j in range(25):
                compound_id = f"{prefix}_{pathway}_{j:03d}"
                exact_mass = 120 + i * 40 + j * 4 + np.random.normal(0, 15)

                compound = {
                    'id': compound_id,
                    'name': f"{pathway}_metabolite_{j}",
                    'formula': f"C{6+j}H{12+j}O{3+i}N{i%2}P{i//3}",
                    'exact_mass': max(80, exact_mass),
                    'pathway': pathway,
                    'pathway_position': j,
                    'biological_role': f"intermediate_{j}"
                }
                compounds.append(compound)

        return compounds

    def search_by_mass(self, query_mz: float, database_name: str, tolerance: float = 0.01) -> List[AnnotationResult]:
        """Search database by mass"""
        if database_name not in self.databases:
            return []

        db = self.databases[database_name]
        matches = []

        for compound in db['compounds']:
            mass_diff = abs(compound['exact_mass'] - query_mz)
            if mass_diff <= tolerance:
                confidence = max(0.1, 1.0 - (mass_diff / tolerance))

                result = AnnotationResult(
                    compound_id=compound['id'],
                    compound_name=compound['name'],
                    formula=compound['formula'],
                    exact_mass=compound['exact_mass'],
                    database=database_name,
                    confidence_score=confidence,
                    match_type='exact' if mass_diff < tolerance/2 else 'isotope',
                    metadata={'mass_error_ppm': (mass_diff / query_mz) * 1e6}
                )
                matches.append(result)

        return sorted(matches, key=lambda x: x.confidence_score, reverse=True)

    def search_all_databases(self, query_mz: float) -> Dict[str, List[AnnotationResult]]:
        """Search all databases for a given mass"""
        all_results = {}

        for db_name in self.databases.keys():
            db_params = self.databases[db_name]['search_params']
            tolerance = db_params.get('mz_tolerance', 0.01)
            results = self.search_by_mass(query_mz, db_name, tolerance)
            if results:
                all_results[db_name] = results[:5]  # Top 5 results per database

        return all_results


class SpectrumEmbeddingEngine:
    """Spectrum embedding and similarity search engine"""

    def __init__(self):
        self.embedding_methods = ['spec2vec', 'stellas', 'fingerprint']
        self.embedding_database = {}

    def spectrum_to_spec2vec(self, spectrum: Spectrum) -> SpectrumEmbedding:
        """Convert spectrum to spec2vec embedding"""
        # Simplified spec2vec implementation
        # Real implementation would use gensim and trained models

        # Create pseudo-word representation
        mz_words = []
        for mz, intensity in zip(spectrum.mz_array, spectrum.intensity_array):
            # Bin m/z values
            mz_bin = int(mz)
            intensity_weight = int(np.log10(max(1, intensity)))

            # Create words based on m/z and intensity
            word = f"mz_{mz_bin}"
            mz_words.extend([word] * intensity_weight)

        # Create embedding vector (simplified - real would use Word2Vec)
        embedding_dim = 100
        embedding = np.random.rand(embedding_dim)  # Placeholder

        # Add some structure based on spectrum properties
        base_peak_mz, base_peak_int = spectrum.base_peak
        embedding[0] = base_peak_mz / 1000.0  # Normalized base peak m/z
        embedding[1] = np.log10(max(1, base_peak_int)) / 10.0  # Log intensity
        embedding[2] = len(spectrum.mz_array) / 1000.0  # Number of peaks
        embedding[3] = spectrum.total_ion_current / 1e6  # TIC

        return SpectrumEmbedding(
            spectrum_id=spectrum.scan_id,
            embedding_vector=embedding,
            embedding_method='spec2vec',
            dimension=embedding_dim,
            metadata={'n_peaks': len(spectrum.mz_array), 'tic': spectrum.total_ion_current}
        )

    def spectrum_to_stellas(self, spectrum: Spectrum) -> SpectrumEmbedding:
        """Convert spectrum to S-Stellas embedding using ambiguous compression"""
        # S-Stellas framework with ambiguous compression

        # Step 1: Ambiguous compression - reduce spectrum to key features
        compressed_features = self._ambiguous_compression(spectrum)

        # Step 2: S-Stellas coordinate transformation
        stellas_coords = self._stellas_transformation(compressed_features)

        # Step 3: Create embedding in S-entropy space
        embedding_dim = 128
        embedding = np.zeros(embedding_dim)

        # Map S-Stellas coordinates to embedding space
        for i, coord in enumerate(stellas_coords[:min(len(stellas_coords), embedding_dim)]):
            embedding[i] = coord

        return SpectrumEmbedding(
            spectrum_id=spectrum.scan_id,
            embedding_vector=embedding,
            embedding_method='stellas',
            dimension=embedding_dim,
            metadata={
                'compression_ratio': len(compressed_features) / len(spectrum.mz_array),
                'stellas_coords': stellas_coords.tolist()
            }
        )

    def _ambiguous_compression(self, spectrum: Spectrum) -> np.ndarray:
        """Ambiguous compression of spectrum to key features"""
        # Group peaks by m/z regions
        mz_bins = np.arange(0, 2000, 10)  # 10 Da bins
        digitized = np.digitize(spectrum.mz_array, mz_bins)

        compressed_features = []
        for bin_idx in range(1, len(mz_bins)):
            mask = digitized == bin_idx
            if np.any(mask):
                # Ambiguous representation: sum of intensities in bin
                bin_intensity = np.sum(spectrum.intensity_array[mask])
                bin_center = mz_bins[bin_idx-1] + 5  # Center of bin
                compressed_features.append([bin_center, bin_intensity])

        return np.array(compressed_features) if compressed_features else np.array([[0, 0]])

    def _stellas_transformation(self, compressed_features: np.ndarray) -> np.ndarray:
        """S-Stellas coordinate transformation"""
        if len(compressed_features) == 0:
            return np.array([0.0])

        # S-Stellas transformation: oscillatory coordinates
        stellas_coords = []

        for i, (mz, intensity) in enumerate(compressed_features):
            # Transform to S-entropy coordinates
            s_coord_1 = np.sin(mz / 100.0) * np.log10(max(1, intensity))  # Oscillatory m/z
            s_coord_2 = np.cos(mz / 200.0) * intensity / 1000.0  # Oscillatory intensity
            s_coord_3 = (mz * intensity) / 1e6  # Combined coordinate

            stellas_coords.extend([s_coord_1, s_coord_2, s_coord_3])

        return np.array(stellas_coords)

    def spectrum_to_fingerprint(self, spectrum: Spectrum) -> SpectrumEmbedding:
        """Convert spectrum to molecular fingerprint embedding"""
        # Create binary fingerprint
        embedding_dim = 256
        fingerprint = np.zeros(embedding_dim)

        # Set bits based on m/z values
        for mz, intensity in zip(spectrum.mz_array, spectrum.intensity_array):
            # Hash m/z to fingerprint position
            bit_position = int(mz * 10) % embedding_dim
            fingerprint[bit_position] = min(1.0, fingerprint[bit_position] + intensity / 10000.0)

        return SpectrumEmbedding(
            spectrum_id=spectrum.scan_id,
            embedding_vector=fingerprint,
            embedding_method='fingerprint',
            dimension=embedding_dim,
            metadata={'fingerprint_type': 'mz_based'}
        )

    def create_embedding(self, spectrum: Spectrum, method: str = 'stellas') -> SpectrumEmbedding:
        """Create spectrum embedding using specified method"""
        if method == 'spec2vec':
            return self.spectrum_to_spec2vec(spectrum)
        elif method == 'stellas':
            return self.spectrum_to_stellas(spectrum)
        elif method == 'fingerprint':
            return self.spectrum_to_fingerprint(spectrum)
        else:
            raise ValueError(f"Unknown embedding method: {method}")

    def similarity_search(self, query_embedding: SpectrumEmbedding,
                         database_embeddings: List[SpectrumEmbedding],
                         top_k: int = 10) -> List[Tuple[SpectrumEmbedding, float]]:
        """Search for similar spectra using embedding similarity"""
        similarities = []

        for db_embedding in database_embeddings:
            if db_embedding.embedding_method == query_embedding.embedding_method:
                # Calculate cosine similarity
                dot_product = np.dot(query_embedding.embedding_vector, db_embedding.embedding_vector)
                norm_query = np.linalg.norm(query_embedding.embedding_vector)
                norm_db = np.linalg.norm(db_embedding.embedding_vector)

                if norm_query > 0 and norm_db > 0:
                    similarity = dot_product / (norm_query * norm_db)
                    similarities.append((db_embedding, similarity))

        # Sort by similarity and return top k
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]


class QualityControlModule:
    """Quality control and statistics for spectra"""

    def __init__(self):
        self.qc_metrics = {}

    def assess_spectrum_quality(self, spectrum: Spectrum) -> Dict[str, float]:
        """Assess quality metrics for a spectrum"""
        metrics = {}

        if len(spectrum.mz_array) == 0:
            return {'quality_score': 0.0, 'noise_level': 1.0}

        # Signal-to-noise ratio
        intensities = spectrum.intensity_array
        signal = np.max(intensities)
        noise = np.median(intensities)
        snr = signal / max(1, noise)

        # Peak density
        mz_range = np.max(spectrum.mz_array) - np.min(spectrum.mz_array)
        peak_density = len(spectrum.mz_array) / max(1, mz_range)

        # Base peak intensity
        base_peak_int = np.max(intensities)

        # Quality score (0-1)
        quality_score = min(1.0, (np.log10(max(1, snr)) / 5.0 +
                                 min(1.0, peak_density / 0.1) +
                                 min(1.0, np.log10(max(1, base_peak_int)) / 6.0)) / 3.0)

        metrics.update({
            'signal_to_noise': snr,
            'peak_density': peak_density,
            'base_peak_intensity': base_peak_int,
            'total_ion_current': spectrum.total_ion_current,
            'peak_count': len(spectrum.mz_array),
            'quality_score': quality_score,
            'noise_level': 1.0 / max(1, snr)
        })

        return metrics

    def dataset_statistics(self, spectra: List[Spectrum]) -> Dict[str, Any]:
        """Calculate comprehensive dataset statistics"""
        if not spectra:
            return {'error': 'No spectra provided'}

        # Basic stats
        n_spectra = len(spectra)
        quality_scores = [self.assess_spectrum_quality(s)['quality_score'] for s in spectra]

        # Peak statistics
        peak_counts = [len(s.mz_array) for s in spectra]
        tics = [s.total_ion_current for s in spectra]
        rt_values = [s.retention_time for s in spectra]

        stats = {
            'dataset_size': n_spectra,
            'quality_metrics': {
                'mean_quality_score': np.mean(quality_scores),
                'std_quality_score': np.std(quality_scores),
                'high_quality_spectra': sum(1 for q in quality_scores if q > 0.7),
                'low_quality_spectra': sum(1 for q in quality_scores if q < 0.3)
            },
            'peak_statistics': {
                'total_peaks': sum(peak_counts),
                'mean_peaks_per_spectrum': np.mean(peak_counts),
                'std_peaks_per_spectrum': np.std(peak_counts),
                'max_peaks': max(peak_counts) if peak_counts else 0,
                'min_peaks': min(peak_counts) if peak_counts else 0
            },
            'intensity_statistics': {
                'mean_tic': np.mean(tics),
                'std_tic': np.std(tics),
                'max_tic': max(tics) if tics else 0,
                'min_tic': min(tics) if tics else 0
            },
            'retention_time_statistics': {
                'rt_range': max(rt_values) - min(rt_values) if rt_values else 0,
                'mean_rt': np.mean(rt_values),
                'std_rt': np.std(rt_values)
            }
        }

        return stats


class FeatureClusteringModule:
    """Feature extraction and clustering for spectra"""

    def __init__(self):
        self.features = {}

    def extract_spectral_features(self, spectrum: Spectrum) -> Dict[str, float]:
        """Extract features from spectrum"""
        if len(spectrum.mz_array) == 0:
            return {}

        features = {}

        # Basic features
        features['base_peak_mz'], features['base_peak_intensity'] = spectrum.base_peak
        features['total_ion_current'] = spectrum.total_ion_current
        features['peak_count'] = len(spectrum.mz_array)

        # Statistical features
        intensities = spectrum.intensity_array
        features['mean_intensity'] = np.mean(intensities)
        features['std_intensity'] = np.std(intensities)
        features['skewness_intensity'] = self._calculate_skewness(intensities)
        features['kurtosis_intensity'] = self._calculate_kurtosis(intensities)

        # m/z features
        mz_values = spectrum.mz_array
        features['mean_mz'] = np.mean(mz_values)
        features['std_mz'] = np.std(mz_values)
        features['mz_range'] = np.max(mz_values) - np.min(mz_values)

        # Spectral entropy
        features['spectral_entropy'] = self._calculate_spectral_entropy(intensities)

        # Top peak ratios
        sorted_intensities = np.sort(intensities)[::-1]
        if len(sorted_intensities) >= 2:
            features['second_peak_ratio'] = sorted_intensities[1] / sorted_intensities[0]
        if len(sorted_intensities) >= 3:
            features['third_peak_ratio'] = sorted_intensities[2] / sorted_intensities[0]

        return features

    def _calculate_skewness(self, values: np.ndarray) -> float:
        """Calculate skewness"""
        if len(values) < 3:
            return 0.0
        mean = np.mean(values)
        std = np.std(values)
        if std == 0:
            return 0.0
        return np.mean(((values - mean) / std) ** 3)

    def _calculate_kurtosis(self, values: np.ndarray) -> float:
        """Calculate kurtosis"""
        if len(values) < 4:
            return 0.0
        mean = np.mean(values)
        std = np.std(values)
        if std == 0:
            return 0.0
        return np.mean(((values - mean) / std) ** 4) - 3

    def _calculate_spectral_entropy(self, intensities: np.ndarray) -> float:
        """Calculate spectral entropy"""
        if len(intensities) == 0 or np.sum(intensities) == 0:
            return 0.0

        # Normalize intensities
        prob = intensities / np.sum(intensities)

        # Calculate entropy
        entropy = -np.sum(prob * np.log2(prob + 1e-10))
        return entropy

    def cluster_spectra(self, spectra: List[Spectrum], n_clusters: int = 5) -> Dict[str, Any]:
        """Cluster spectra based on extracted features"""
        if len(spectra) < n_clusters:
            n_clusters = len(spectra)

        # Extract features for all spectra
        feature_vectors = []
        spectrum_ids = []

        for spectrum in spectra:
            features = self.extract_spectral_features(spectrum)
            if features:
                feature_vector = list(features.values())
                feature_vectors.append(feature_vector)
                spectrum_ids.append(spectrum.scan_id)

        if len(feature_vectors) == 0:
            return {'error': 'No features extracted'}

        # Simple k-means clustering (without sklearn)
        feature_matrix = np.array(feature_vectors)

        # Normalize features
        feature_matrix = (feature_matrix - np.mean(feature_matrix, axis=0)) / (np.std(feature_matrix, axis=0) + 1e-10)

        # Simple k-means implementation
        clusters = self._simple_kmeans(feature_matrix, n_clusters)

        # Organize results
        cluster_results = defaultdict(list)
        for spectrum_id, cluster_id in zip(spectrum_ids, clusters):
            cluster_results[f'cluster_{cluster_id}'].append(spectrum_id)

        return {
            'n_clusters': n_clusters,
            'n_spectra_clustered': len(spectrum_ids),
            'cluster_assignments': dict(cluster_results),
            'cluster_sizes': {f'cluster_{i}': len(cluster_results[f'cluster_{i}'])
                            for i in range(n_clusters)}
        }

    def _simple_kmeans(self, data: np.ndarray, k: int, max_iters: int = 100) -> np.ndarray:
        """Simple k-means implementation"""
        n_samples, n_features = data.shape

        # Initialize centroids randomly
        centroids = data[np.random.choice(n_samples, k, replace=False)]

        for _ in range(max_iters):
            # Assign points to closest centroids
            distances = np.sqrt(((data - centroids[:, np.newaxis])**2).sum(axis=2))
            closest_cluster = np.argmin(distances, axis=0)

            # Update centroids
            new_centroids = np.array([data[closest_cluster == i].mean(axis=0)
                                    for i in range(k)])

            # Check for convergence
            if np.allclose(centroids, new_centroids):
                break

            centroids = new_centroids

        return closest_cluster


class NumericalPipelineOrchestrator:
    """Main numerical pipeline orchestrator with Oscillatory Hierarchy Navigation"""
    
    def __init__(self):
        self.mzml_reader = StandaloneMzMLReader()
        self.database_search = DatabaseSearchEngine()
        self.embedding_engine = SpectrumEmbeddingEngine()
        self.quality_control = QualityControlModule()
        self.feature_clustering = FeatureClusteringModule()
        
        # Revolutionary Oscillatory Hierarchy Integration
        from .oscillatory_hierarchy import create_oscillatory_hierarchy
        self.oscillatory_hierarchy = create_oscillatory_hierarchy()
        self.use_oscillatory_navigation = True

    def process_dataset(self, mzml_filepath: str,
                       use_stellas: bool = True,
                       min_quality: float = 0.3) -> Dict[str, Any]:
        """
        Complete numerical processing pipeline

        Args:
            mzml_filepath: Path to mzML file
            use_stellas: Whether to use S-Stellas framework
            min_quality: Minimum quality threshold for spectra

        Returns:
            Complete processing results
        """
        start_time = time.time()

        print(f"Starting numerical processing of {mzml_filepath}")

        # Step 1: Load mzML file
        print("Step 1: Loading mzML file...")
        spectra = self.mzml_reader.load_mzml(mzml_filepath)
        dataset_summary = self.mzml_reader.get_dataset_summary(spectra)
        
        # REVOLUTIONARY: Build Oscillatory Hierarchy for O(1) Navigation
        print("Step 1b: Building Oscillatory Hierarchy...")
        from .oscillatory_hierarchy import add_spectra_to_hierarchy, HierarchicalLevel
        spectrum_mapping = add_spectra_to_hierarchy(self.oscillatory_hierarchy, spectra)
        hierarchy_stats = self.oscillatory_hierarchy.get_hierarchy_statistics()
        print(f"âœ“ Built hierarchy with {hierarchy_stats['total_nodes']} nodes across {len(hierarchy_stats['nodes_per_level'])} levels")

        # Step 2: Quality control filtering
        print("Step 2: Quality control assessment...")
        qc_stats = self.quality_control.dataset_statistics(spectra)

        # Filter by quality
        high_quality_spectra = []
        for spectrum in spectra:
            quality_metrics = self.quality_control.assess_spectrum_quality(spectrum)
            if quality_metrics['quality_score'] >= min_quality:
                high_quality_spectra.append(spectrum)

        print(f"Filtered to {len(high_quality_spectra)} high-quality spectra")

        # Step 3: Revolutionary Oscillatory Navigation vs Traditional Database Search
        print("Step 3: Oscillatory Hierarchy Navigation vs Traditional Database Annotation...")
        annotation_results = {}
        oscillatory_navigation_results = {}
        
        # Use MS1 spectra for annotation
        ms1_spectra = [s for s in high_quality_spectra if s.ms_level == 1][:20]  # Limit for demo
        
        # TRADITIONAL APPROACH (O(N) complexity)
        traditional_start = time.time()
        for spectrum in ms1_spectra:
            base_peak_mz, _ = spectrum.base_peak
            annotations = self.database_search.search_all_databases(base_peak_mz)
            if annotations:
                annotation_results[spectrum.scan_id] = annotations
        traditional_time = time.time() - traditional_start
        
        # REVOLUTIONARY OSCILLATORY NAVIGATION (O(1) complexity)
        oscillatory_start = time.time()
        from .oscillatory_hierarchy import navigate_hierarchy_o1
        
        for spectrum in ms1_spectra:
            # Navigate using gear ratios instead of linear search
            
            # Find similar spectra using O(1) navigation
            similar_spectra = navigate_hierarchy_o1(
                self.oscillatory_hierarchy,
                spectrum.scan_id,
                {'mass_range': self._classify_mass_range(spectrum.base_peak[0])},
                HierarchicalLevel.SPECTRUM
            )
            
            # Navigate to instrument class
            instrument_matches = navigate_hierarchy_o1(
                self.oscillatory_hierarchy,
                spectrum.scan_id,
                {},
                HierarchicalLevel.INSTRUMENT_CLASS
            )
            
            # Use St-Stellas Molecular Language for semantic annotation
            stellas_encoding = self.oscillatory_hierarchy.stellas_language.encode_molecular_structure({
                'molecular_class': self._infer_molecular_class(spectrum),
                'exact_mass': spectrum.base_peak[0],
                'formula': self._estimate_formula(spectrum),
                'polarity': spectrum.polarity
            })
            
            oscillatory_navigation_results[spectrum.scan_id] = {
                'similar_spectra_count': len(similar_spectra),
                'instrument_matches': len(instrument_matches),
                'stellas_encoding': stellas_encoding,
                'semantic_amplification': stellas_encoding.get('semantic_distance_amplification', 1.0),
                'navigated_using_gear_ratios': True
            }
        
        oscillatory_time = time.time() - oscillatory_start
        
        # Calculate performance improvement
        performance_improvement = traditional_time / max(oscillatory_time, 0.001)  # Avoid division by zero
        print(f"ðŸš€ PERFORMANCE BREAKTHROUGH:")
        print(f"   Traditional Database Search: {traditional_time:.4f}s")
        print(f"   Oscillatory Navigation: {oscillatory_time:.4f}s") 
        print(f"   Speed Improvement: {performance_improvement:.1f}x faster!")

        # Step 4: Spectrum embeddings
        print("Step 4: Creating spectrum embeddings...")
        embeddings = {}

        embedding_methods = ['stellas', 'spec2vec'] if use_stellas else ['spec2vec']

        for method in embedding_methods:
            method_embeddings = []
            for spectrum in high_quality_spectra[:50]:  # Limit for demo
                embedding = self.embedding_engine.create_embedding(spectrum, method)
                method_embeddings.append(embedding)
            embeddings[method] = method_embeddings

        # Similarity search example
        similarity_results = {}
        if len(embeddings.get('stellas', [])) > 1:
            query_embedding = embeddings['stellas'][0]
            database_embeddings = embeddings['stellas'][1:]
            similar_spectra = self.embedding_engine.similarity_search(
                query_embedding, database_embeddings, top_k=5
            )
            similarity_results['stellas_similarity'] = [
                {'spectrum_id': emb.spectrum_id, 'similarity': sim}
                for emb, sim in similar_spectra
            ]

        # Step 5: Feature clustering
        print("Step 5: Feature clustering...")
        clustering_results = self.feature_clustering.cluster_spectra(high_quality_spectra[:100], n_clusters=5)

        processing_time = time.time() - start_time

        # Compile results with Revolutionary Oscillatory Navigation
        results = {
            'pipeline_info': {
                'input_file': mzml_filepath,
                'processing_time': processing_time,
                'use_stellas': use_stellas,
                'use_oscillatory_navigation': self.use_oscillatory_navigation,
                'min_quality_threshold': min_quality
            },
            'dataset_summary': dataset_summary,
            'quality_control': qc_stats,
            'spectra_processed': {
                'total_input': len(spectra),
                'high_quality': len(high_quality_spectra),
                'ms1_count': len([s for s in high_quality_spectra if s.ms_level == 1]),
                'ms2_count': len([s for s in high_quality_spectra if s.ms_level == 2])
            },
            'revolutionary_oscillatory_hierarchy': {
                'hierarchy_stats': hierarchy_stats,
                'spectrum_mapping_count': len(spectrum_mapping),
                'gear_ratio_navigation_enabled': True,
                'transcendent_observer_active': True
            },
            'annotation_comparison': {
                'traditional_database_search': {
                    'total_annotated_spectra': len(annotation_results),
                    'processing_time': traditional_time,
                    'complexity': 'O(N)',
                    'annotations_per_database': {
                        db: sum(1 for annotations in annotation_results.values() 
                               if db in annotations)
                        for db in self.database_search.databases.keys()
                    },
                    'sample_annotations': dict(list(annotation_results.items())[:3])
                },
                'oscillatory_navigation': {
                    'total_navigated_spectra': len(oscillatory_navigation_results),
                    'processing_time': oscillatory_time,
                    'complexity': 'O(1)',
                    'performance_improvement_factor': performance_improvement,
                    'semantic_amplification_stats': {
                        'mean_amplification': np.mean([
                            result.get('semantic_amplification', 1.0) 
                            for result in oscillatory_navigation_results.values()
                        ]) if oscillatory_navigation_results else 1.0,
                        'max_amplification': max([
                            result.get('semantic_amplification', 1.0) 
                            for result in oscillatory_navigation_results.values()
                        ]) if oscillatory_navigation_results else 1.0
                    },
                    'stellas_molecular_language_usage': len(oscillatory_navigation_results),
                    'sample_navigation_results': dict(list(oscillatory_navigation_results.items())[:3])
                }
            },
            'spectrum_embeddings': {
                'methods_used': list(embeddings.keys()),
                'embeddings_per_method': {method: len(emb_list) 
                                        for method, emb_list in embeddings.items()},
                'embedding_dimensions': {method: emb_list[0].dimension if emb_list else 0
                                       for method, emb_list in embeddings.items()},
                'similarity_search_results': similarity_results
            },
            'feature_clustering': clustering_results
        }

        print(f"Numerical processing completed in {processing_time:.2f} seconds")
        
        return results
    
    def _classify_mass_range(self, mass: float) -> str:
        """Classify mass into ranges for hierarchical navigation"""
        if mass < 300:
            return "low_mass"
        elif mass < 800:
            return "medium_mass"
        else:
            return "high_mass"
    
    def _infer_molecular_class(self, spectrum: Spectrum) -> str:
        """Infer molecular class from spectrum characteristics"""
        base_peak_mz, base_peak_intensity = spectrum.base_peak
        
        # Simple heuristic based on mass and polarity
        if spectrum.polarity == 'positive':
            if 700 <= base_peak_mz <= 900:
                return 'PC'  # Phosphatidylcholine
            elif 600 <= base_peak_mz <= 800:
                return 'PE'  # Phosphatidylethanolamine
            elif 800 <= base_peak_mz <= 1000:
                return 'TG'  # Triglyceride
            elif base_peak_mz < 300:
                return 'amino_acid'
        else:  # negative polarity
            if 700 <= base_peak_mz <= 900:
                return 'PS'  # Phosphatidylserine
            elif 600 <= base_peak_mz <= 800:
                return 'PA'  # Phosphatidic acid
            elif base_peak_mz < 300:
                return 'organic_acid'
        
        return 'unknown_molecule'
    
    def _estimate_formula(self, spectrum: Spectrum) -> str:
        """Estimate molecular formula from spectrum"""
        base_peak_mz, _ = spectrum.base_peak
        
        # Very simplified formula estimation
        # In reality, this would use sophisticated algorithms
        
        # Estimate carbon count from mass
        estimated_carbons = int(base_peak_mz / 12)  # Rough estimate
        estimated_hydrogens = estimated_carbons * 2  # Saturated approximation
        
        # Adjust based on polarity and inferred class
        mol_class = self._infer_molecular_class(spectrum)
        
        if mol_class in ['PC', 'PE', 'PS']:
            # Phospholipid formula pattern
            formula = f"C{estimated_carbons}H{estimated_hydrogens}NO8P"
        elif mol_class == 'TG':
            # Triglyceride formula pattern
            formula = f"C{estimated_carbons}H{estimated_hydrogens}O6"
        elif mol_class == 'amino_acid':
            # Amino acid formula pattern
            formula = f"C{min(20, estimated_carbons)}H{estimated_hydrogens}NO2"
        else:
            # Generic organic formula
            formula = f"C{estimated_carbons}H{estimated_hydrogens}O"
        
        return formula


# Convenience functions for validation framework
def create_numerical_validator() -> NumericalPipelineOrchestrator:
    """Create a numerical pipeline validator"""
    return NumericalPipelineOrchestrator()


def process_mzml_numerical(filepath: str, stellas_transform: bool = True) -> Dict[str, Any]:
    """
    Process mzML file through numerical pipeline

    Args:
        filepath: Path to mzML file
        stellas_transform: Whether to use S-Stellas framework

    Returns:
        Processing results dictionary
    """
    orchestrator = NumericalPipelineOrchestrator()
    return orchestrator.process_dataset(filepath, use_stellas=stellas_transform)


if __name__ == "__main__":
    # Test the numerical pipeline
    test_files = ["PL_Neg_Waters_qTOF.mzML", "TG_Pos_Thermo_Orbi.mzML"]

    orchestrator = NumericalPipelineOrchestrator()

    for test_file in test_files:
        print(f"\n{'='*60}")
        print(f"Testing numerical pipeline with: {test_file}")
        print('='*60)

        results = orchestrator.process_dataset(test_file, use_stellas=True)

        # Print summary
        print(f"\nProcessing Summary:")
        print(f"Processing time: {results['pipeline_info']['processing_time']:.2f} seconds")
        print(f"Input spectra: {results['spectra_processed']['total_input']}")
        print(f"High-quality spectra: {results['spectra_processed']['high_quality']}")
        print(f"Annotated spectra: {results['database_annotations']['total_annotated_spectra']}")

        embedding_info = results['spectrum_embeddings']
        print(f"Embedding methods: {embedding_info['methods_used']}")
        for method in embedding_info['methods_used']:
            count = embedding_info['embeddings_per_method'][method]
            dim = embedding_info['embedding_dimensions'][method]
            print(f"  {method}: {count} embeddings (dim={dim})")

        clustering_info = results['feature_clustering']
        if 'n_clusters' in clustering_info:
            print(f"Clustering: {clustering_info['n_clusters']} clusters for {clustering_info['n_spectra_clustered']} spectra")
